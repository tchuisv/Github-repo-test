Kubernetes

Container Orchestration plateform that automates  deployment

Pradeep Kumar
9:06 PM
---------------------------DOCKER CONTAINERS ---------------------------------------------

Docker is a platform which provides Containerization

What is Containerization?
It is a process of creating Containers

Why do we need DOCKER CONTAINERS:-
TO run our applications -> Run/creating images, containers

------------------------------ KUBERNETES PODS --------------------------------------------

What is KUBERNETES?
It is a Container Orchestration platform that Automates deployment of Containers
------------------------------ ADVANTAGES/REALTIME APPLICATIONS of KUBERNETES -------------------------------

KUBERNETES ADMINISTRATOR/DEVELOPER

GOOGLE Team is working on different solutions for managing the PRODUCTION level CONTAINERS/PODS/CLUSTERS

KUBERNETES -> ORGANIZATION (PRODUCT)

PROBLEMS which we may face using DOCKER CONTAINERS: -
1. Auto Scaling -> Increasing or Decreasing number of CONTAINERS/PODS based on the DEMAND from the Customers
2. Auto Healing -> If Container dies, it should revive Automatically, using REPLICA SETS
3. LOAD BALANCER -> Equally distributing the Traffic across different Containers
4. ENTERPRISE LEVEL Solution -> Docker do not provide ENTERPRISE level solutions
-> KUBENETES is Free 
-> AZURE KUBERNETES Service (AKS)
-> Elastic Kubernetes Service (EKS) available in AWS
-> FIREWALLS, Security 


Pradeep Kumar
9:05 PM
How do we AUTO SCALE ?

HORIZONTAL POD Auto scaling : -
-> You increase/decrease the number of PODS 
-> Minimum number of REPLICAS/PODS and Maximum number of REPLICAS/PODS

What is difference between POD and REPLICA Set ?
-> POD is the actually a Container and REPLICA set is a copy of a POD

Cluster level Auto scaling: -
Based on number of worker nodes

-> No of worker nodes
-> Maximum capacity of your worker nodes (8GB RAM and 30 GB)
-> Maximum capacity used by your worker node (6gb and 25GB)
-------------------------------------- KUBERNETES ARCHITECTURE  --------------------------------------

Data Plane or Worker Nodes : -
-> Kubelet : Responsible for creating and running the PODS and it informs Control plane if a POD is not available
-> Kube-proxy : Responsible for assigning the IP address and it diverts the traffic through LOAD Balancers for PODS
-> PODS : One or more COntainers make a POD


Control Plane or Master Node
-> API server: Decides on which Worker node the PODS should
-> Scheduler : It will schedule the PODS creation
-> ETCD : Entire cluster information is stored in Key-value pair format


KOP: Kubernetes opertions used for creating clusters on the cloud

KubeCTL: usered ot configuration of the cluster, pODS, tbshoot and service

Minikube:
used for local kubernetes cluster creation. Single node clusters



Pradeep Kumar
9:04 PM
What is KOPS ?
Kubernetes Operations.
-> It is a 3rd party tool
-> Creation of Kubernetes clusters on CLOUD

What is MINIKUBE ?
-> Used for local Kubernetes cluster creation 
-> Single node clusters on your local machine
-> Only one EC2 instance or you can install on your LAPTOP

What is KubeCTL ?
-> It is used for Configuration of the CLusters
-> We can create PODS, services, troubleshoot
FIRST APPROACH : -
-> Create an EC2 instance and install Kubernetes on EC2 using any of the above tools
-> IF something goes wrong, AMAZON will not provide the support for Kubernetes
-> This is Cost efficient
-> Timelines are not stringent/short for fixing the error

LINUX DISRIBUTIONS: -
Amazon Linux Distributions
Redhat
CentOS
SECOND APPROACH is KUBENERNETES DISTRIBUTIONS: -
Kubernetes is an OPENSOURCE TOOL: -
-> EKS (Elastic Kubernetes Ser
-> RANCHER 
-> Timelines are stringent/short forvice) -> AWS will provide the servers/machines with already installing Kubernetes
-> AKS (Azure Kubernetes Service)
-> Openshift
-> Tanzu fixing the error
-> IN DOcker, we have CONTAINERS as the Lowest level deployment
-> In K8s, we have PODS as lowest level of deployment

-> One or more containers make a POD
-> We have YAML file in k8s, for creating PODS 

POD.yml
-> Your containername
-> Your image name
-> Your image port number
-> Base image details


-------------------------------------------------------------------------------------------------------------

Create a POD using 2 containers 

-> Shared networking
-> shared storage



Running local CLuster

Pradeep Kumar
9:02 PM
STEP-1 : - Update the Ubuntu OS and switch to root user
-> sudo apt update
-> sudo -i

STEP-2 : Install Docker
  sudo apt -y install docker.io
STEP-3 : Install Kubernetes packages
  -> curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl
  -> curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
  -> apt install conntrack
STEP-4 : Add the current user to the docker group and start a new shell with the updated group membership.
-> sudo usermod -aG docker $USER && newgrp docker

STEP-5 : Start Minikube  
-> minikube start --driver=docker
Pradeep Kumar
9:06 PM
STEP-6 : Deploy a pod with the react todolist app image
  kubectl run todolistapp --image=kubekode/react-todo-list-app

  Pradeep Kumar
9:07 PM
STEP -7 : Deploy a NodePort service for this pod.
  kubectl expose pod todolistapp --type=NodePort --port=80 --name=todolistapp-service

STEP - 8 : Check service is working or not
  minikube service todolistapp-service --url

STEP - 9 : Request the URL using curl to check it's working or not.
  curl <URL>

  STEP -10 :  Forward the traffic from 3000 port to 80 port on our pod.
  kubectl port-forward svc/todolistapp-service 3000:80 --address 0.0.0.0 &
  
  
   Add port 3000 in security groups if you face any error







Commands
 1  minikube start --driver=docker
    2  minikube status
    3  history
ubuntu@ip-172-31-21-85:~$ sudo su
root@ip-172-31-21-85:/home/ubuntu# history
    1  minikube start --driver=docker
    2  exit
    3  curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl && chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl
    4  curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
    5  apt install conntrack
    6  sudo usermod -aG docker $USER && newgrp docker
    7  rxit
    8  whoami
    9  exit
   10  history

    1  minikube status
    2  docker ps

    3  kubectl run todolistapp --image=kubekode/react-todo-list-app
    4  kubectl expose pod todolistapp --type=NodePort --port=80 --name=todolistapp-service
    5  minikube service todolistapp-service --url
      kubectl get service
      kubectl get pods
    6  kubectl port-forward svc/todolistapp-service 3000:80 --address 0.0.0.0 &
    7  docker ps
    8  docker images
    9  history



    Autoscaling

    AUTO SCALING: -

Bateisa Gaynor
9:07 PM
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
PRADEEP
9:11 PM
AUTO SCALING: -

What is Auto-scaling?
Increasing or Decreasing the number of PODS or Nodes


-> Horizontal POD Auto scaling (HPA)
We increase or decrease the number of PODS

-> Vertical POD Auto scaling (VPA)
Increase the resources like memory or CPU or storage

-> Cluster Autoscaling (CA)
Increase the nodes/servers
----------------------------------CONTAINER & POD & DEPLOYMENT----------------------------------

CONTAINER : -
WE use docker images for running the Containers

PODS: -
-> We create PODS using containers
-> One or more containers make a POD
-> Image name, container name, port number

DEPLOYMENT:-
-> It manages or handles PODS or complete lifecycle
-> we should deploy our applications in PROD as deployment
-> Image details, port number, POD details, Load balancer details, Auto scaling details, M
----------------------------------- HORIZONTAL POD AUTO SCALING ---------------------------------------------

What is the minimum number of REPLICAS or PODS that are required in HPA ?
Minimum PODS or Replicas -> 1 or more

What is the maximum number of REPLICAS or PODS that are required in HPA ?
Maximum PODS or REPLICAS -> 

-> Current or Actual PODS that are running 
-> What is the current resource usage in percentage of the server (Memory, CPU, storage)
-> what is the target limit that we ha
Desired no. of REPLICAS = Actual PODS * (Current % of usage/Target limit of Usage)

Desired no. of REPLICAS = 3 * ( 90 / 70 % )



https://github.com/pradeepV2/Autoscaling-project
Bateisa Gaynor
8:37 PM
To determine the number of replicas based on load, you can perform the following calculation:

Number of Replicas
=
Total Expected Load (requests/second)
Max Requests/Replica
Number of Replicas= 
Max Requests/Replica
Total Expected Load (requests/second)
​
 
Steps:
Estimate the total expected load (e.g., requests per second or transactions).
Measure how many requests or tasks a single pod can handle efficiently (without exhausting CPU, memory, etc.).
Divide the total load by the capacity of a single pod to determine the number of replicas needed.
Bateisa Gaynor
8:51 PM
LoadBalancer:
Use Case: When you want to expose your application externally in cloud environments (AWS, GCP, Azure) with automatic load balancing.
Description: The service provisions an external LoadBalancer in cloud environments. The LoadBalancer routes traffic from outside the cluster to the service, which then directs traffic to the Pods.
Benefit: It hides direct access to the Pods, provides high availability, and manages external traffic routing automatically.
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
    - port: 80
      targetPort: 8080



      Running cluster with yml file

          1  minikube start --driver=docker
    2  sudo apt install git
    3  git clone https://github.com/tchuisv/Autoscaling-project.git
    4  ls
    5  cd Autoscaling-project/
    6  ls
    7  cd HPA/
    8  ls
    9  kubectl get pods
   10  kubectl apply -f 01_Deployment.yml 
   11  kubectl get pods
   12  kubectl describe pod hpa-demo-deployment-9cc6d54b5-xd6x6
   13  kubectl get service
   14  kubectl get pods
   15  kubectl service
   16  kubectl get service
   17  kubectl expose pod hpa-demo-deployment-9cc6d54b5-xd6x6 --type=NodePort --port=80 --name autoscaleservice
   18  kubectl get service
   19  minikube service autoscaleservice --url
   20  kubectl port-forward svc/autoscaleservice 3000:80 --address 0.0.0.0 &
   21  kubectl get pods
   22  kubectl get service
   23  history



https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

STEP 3: - Create the Cluster using minikube
-> minikube start --nodes 2 -p multinode-cluster
Pradeep
8:33 PM
Refer the minikube documentation if required.
https://minikube.sigs.k8s.io/docs/tutorials/multi_node/kubec


https://minikube.sigs.k8s.io/docs/tutorials/multi_node/


Pradeep
8:33 PM
Refer the minikube documentation if required.
https://minikube.sigs.k8s.io/docs/tutorials/multi_node/
Pradeep
8:38 PM
METRICS SERVER Installation: -
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.2/components.yaml
Pradeep
9:02 PM
STEP 7 : - Verify the created services
kubectl get all

Wait for the HPA to kickin 

Check the created items
kubectl get all
STEP 8 : Increase the LOAD on POD using load generator
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"

Check HPA: -
kubectl get hpa

Check Replica sets: -
kubectl get rs

Check pods: -
kubectl get pods

Check HPA: -
kubectl get hpa

Check all: -
kubectl get all

STOP the load on terminal where load is runnin
CTRL + C + exit
Pradeep
9:03 PM
https://minikube.sigs.k8s.io/docs/tutorials/multi_node/



Helm installation

 https://helm.sh/docs/intro/install/  


STEPS for HELM Project: -

1. Update the Ubuntu machine and switch to the root user
-> sudo apt update
-> sudo su

2. Install Helm on your Ubuntu machine
-> curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
-> sudo apt-get install apt-transport-https --yes
-> echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
-> sudo apt-get update
-> sudo apt-get install helm


3. Create the HELM Application
-> helm create webapp1



